#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MedMNIST æ•°æ®é›†ä¸‹è½½ä¸è½¬æ¢ï¼ˆé€‚é… MedMamba / ImageFolder-224ï¼‰
- å¼ºåˆ¶ RGB ä¸‰é€šé“
- ç¨³å¥æ ‡ç­¾è½¬æ¢ï¼ˆint / array / one-hotï¼‰
- ä¸åšé”™è¯¯çš„åå½’ä¸€åŒ–
- å†™ labels.json è®°å½•ç±»é¡ºåºä¸å®˜æ–¹ç±»å
"""

import os
import sys
import argparse
import json
import numpy as np
from PIL import Image
from tqdm import tqdm

try:
    import medmnist
    from medmnist import INFO, PathMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, \
                         RetinaMNIST, BreastMNIST, BloodMNIST, OrganAMNIST, \
                         OrganCMNIST, OrganSMNIST
except ImportError:
    print("âŒ é”™è¯¯: æœªå®‰è£… medmnist åŒ…ï¼Œè¯·å…ˆè¿è¡Œ: pip install medmnist")
    sys.exit(1)

# æ”¯æŒçš„æ•°æ®é›†ï¼ˆå•æ ‡ç­¾ä»»åŠ¡ï¼‰
DATASET_MAP = {
    # 'pathmnist': PathMNIST,
    # 'dermamnist': DermaMNIST,
    # 'octmnist': OCTMNIST,
    # 'pneumoniamnist': PneumoniaMNIST,
    # 'retinamnist': RetinaMNIST,
    # 'breastmnist': BreastMNIST,
    # 'bloodmnist': BloodMNIST,
    'organamnist': OrganAMNIST,
    # 'organcmnist': OrganCMNIST,
    # 'organsmnist': OrganSMNIST,
}

def print_dataset_info():
    print("\n" + "=" * 80)
    print("MedMNIST æ•°æ®é›†ä¿¡æ¯".center(80))
    print("=" * 80)
    for idx, (dataset_name, info) in enumerate(INFO.items(), 1):
        if dataset_name in DATASET_MAP:
            label_names = info.get('label', {})
            print(f"\n{idx}. {dataset_name.upper()}")
            print(f"   â”œâ”€ ä»»åŠ¡ç±»å‹: {info['task']}")
            print(f"   â”œâ”€ ç±»åˆ«æ•°é‡: {len(label_names)}")
            print(f"   â”œâ”€ å›¾åƒå°ºå¯¸: 28x28 (å°†è½¬æ¢ä¸º 224x224 RGB)")
            print(f"   â””â”€ ç±»åˆ«åç§°: {label_names}")
    print("\n" + "=" * 80 + "\n")

def _ensure_pil_rgb(img):
    """å°†å„ç§è¾“å…¥ï¼ˆPIL/ndarray/torch.Tensorï¼‰ç¨³å¥è½¬æ¢ä¸º RGB PIL.Imageã€‚"""
    try:
        import torch
        is_tensor = torch.is_tensor(img)
    except Exception:
        is_tensor = False

    if is_tensor:
        img = img.detach().cpu().numpy()

    if isinstance(img, np.ndarray):
        # æ•°å€¼åŸŸå¯èƒ½æ˜¯ [0,1] æˆ– [0,255]
        if img.ndim == 2:  # H,W
            arr = img
        elif img.ndim == 3:
            # (C,H,W) æˆ– (H,W,C)
            if img.shape[0] in (1,3) and img.ndim == 3:
                # å‡å®š (C,H,W)
                arr = np.transpose(img, (1, 2, 0))
            else:
                arr = img
        else:
            raise ValueError(f"Unsupported ndarray shape: {img.shape}")

        # å½’ä¸€åŒ–åˆ° 0â€“255 uint8
        if arr.dtype != np.uint8:
            arr = arr.astype(np.float32)
            if arr.max() <= 1.0:
                arr = (arr * 255.0).round()
            arr = np.clip(arr, 0, 255).astype(np.uint8)

        if arr.ndim == 2:
            pil = Image.fromarray(arr, mode="L")
        elif arr.shape[2] == 1:
            pil = Image.fromarray(arr.squeeze(2), mode="L")
        elif arr.shape[2] == 3:
            pil = Image.fromarray(arr, mode="RGB")
        else:
            # è¶…è¿‡ 3 é€šé“ï¼Œå–å‰ä¸‰ä¸ªé€šé“
            pil = Image.fromarray(arr[:, :, :3], mode="RGB")
        return pil.convert("RGB")

    # PIL.Image
    try:
        from PIL.Image import Image as PILImage
        if isinstance(img, PILImage):
            return img.convert("RGB")
    except Exception:
        pass

    # å…œåº•ï¼šå†æ¬¡å°è¯•ä» numpy è½¬
    return _ensure_pil_rgb(np.array(img))

def _label_to_int(label):
    """å°† medmnist çš„ labelï¼ˆint/np.array/one-hotï¼‰è½¬ä¸º intã€‚"""
    if isinstance(label, (int, np.integer)):
        return int(label)
    if hasattr(label, 'item'):
        try:
            return int(label.item())
        except Exception:
            pass
    label = np.array(label)
    if label.ndim == 0:
        return int(label)
    # one-hot æˆ– shape (1,)
    return int(label.argmax())

def convert_to_imagefolder(dataset, output_dir, split_name):
    """å°† MedMNIST æ•°æ®é›†è½¬æ¢ä¸º ImageFolder(split/class_i/xxx.png)"""
    split_dir = os.path.join(output_dir, split_name)
    os.makedirs(split_dir, exist_ok=True)

    # ç»Ÿè®¡ç±»åˆ«æ•°ï¼ˆä»å…ƒä¿¡æ¯è€Œé labels æ¨æ–­æ›´ç¨³ï¼‰
    # éƒ¨åˆ†æ•°æ®é›† labels å¯èƒ½ä¸å…¨è¦†ç›–ï¼›ä» INFO ä½¿ç”¨å®˜æ–¹ç±»æ•°
    dataset_name = dataset.__class__.__name__.replace('MNIST', '').lower() + 'mnist'
    if dataset_name not in INFO:
        # å›é€€ï¼šç”¨æ•°æ®é‡Œçš„ labels
        num_classes = int(len(np.unique(dataset.labels)))
    else:
        num_classes = int(len(INFO[dataset_name]['label']))

    # åˆ›å»ºç±»åˆ«ç›®å½• class_0 ... class_{K-1}
    for c in range(num_classes):
        os.makedirs(os.path.join(split_dir, f"class_{c}"), exist_ok=True)

    # éå†ä¿å­˜
    for idx in tqdm(range(len(dataset)), desc=f"   {split_name}", leave=False):
        img, label = dataset[idx]
        img_pil = _ensure_pil_rgb(img).resize((224, 224), Image.BILINEAR)
        y = _label_to_int(label)
        y = max(0, min(num_classes - 1, y))  # clamp é˜²è¶Šç•Œ
        out_path = os.path.join(split_dir, f"class_{y}", f"{split_name}_{idx:06d}.png")
        img_pil.save(out_path)

    return num_classes

def download_dataset(dataset_key, output_base_dir, raw_only=False):
    """ä¸‹è½½å¹¶è½¬æ¢æŒ‡å®šçš„ MedMNIST æ•°æ®é›†"""
    dataset_key = dataset_key.lower()
    if dataset_key not in DATASET_MAP:
        print(f"âŒ é”™è¯¯: æœªçŸ¥æ•°æ®é›† '{dataset_key}'ï¼Œå¯ç”¨: {list(DATASET_MAP.keys())}")
        return False

    print("\n" + "=" * 80)
    print(f"ä¸‹è½½æ•°æ®é›†: {dataset_key.upper()}".center(80))
    print("=" * 80 + "\n")

    DataClass = DATASET_MAP[dataset_key]

    raw_dir = os.path.join(output_base_dir, 'raw')
    os.makedirs(raw_dir, exist_ok=True)

    try:
        # ç›´æ¥è¦æ±‚ RGB ä¸‰é€šé“
        print("ğŸ“¥ ä¸‹è½½åŸå§‹æ•°æ® (as_rgb=True)...")
        train_ds = DataClass(split='train', download=True, root=raw_dir, as_rgb=True)
        val_ds   = DataClass(split='val',   download=True, root=raw_dir, as_rgb=True)
        test_ds  = DataClass(split='test',  download=True, root=raw_dir, as_rgb=True)

        print(f"   âœ“ è®­ç»ƒ: {len(train_ds)}   âœ“ éªŒè¯: {len(val_ds)}   âœ“ æµ‹è¯•: {len(test_ds)}")

        if raw_only:
            print(f"\nâœ… åŸå§‹æ•°æ®ä¸‹è½½å®Œæˆ -> {raw_dir}")
            return True

        # è½¬ ImageFolder-224
        print("\nğŸ”„ è½¬æ¢ä¸º ImageFolder-224 ...")
        imagefolder_dir = os.path.join(output_base_dir, 'imagefolder_224')
        K_train = convert_to_imagefolder(train_ds, imagefolder_dir, 'train')
        K_val   = convert_to_imagefolder(val_ds,   imagefolder_dir, 'val')
        K_test  = convert_to_imagefolder(test_ds,  imagefolder_dir, 'test')
        assert K_train == K_val == K_test, "è®­ç»ƒ/éªŒè¯/æµ‹è¯• ç±»åˆ«æ•°ä¸ä¸€è‡´"

        # å†™ labels.jsonï¼ˆç±»é¡ºåºä¸æ–‡ä»¶å¤¹é¡ºåºä¸€è‡´ï¼‰
        labels_json = {
            "dataset": dataset_key,
            "num_classes": K_train,
            "folder_classes": [f"class_{i}" for i in range(K_train)],
            "official_class_names": INFO[dataset_key].get("label", {}),
        }
        with open(os.path.join(imagefolder_dir, "labels.json"), "w") as f:
            json.dump(labels_json, f, indent=2, ensure_ascii=False)

        # å†™ dataset_info.txt
        info_file = os.path.join(imagefolder_dir, 'dataset_info.txt')
        with open(info_file, 'w') as f:
            f.write(f"æ•°æ®é›†: {dataset_key}\n")
            f.write(f"ç±»åˆ«æ•°: {K_train}\n")
            f.write(f"å›¾åƒ: 224x224 RGB\n")
            f.write(f"è®­ç»ƒ/éªŒè¯/æµ‹è¯•: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\n\n")
            f.write("å®˜æ–¹ä¿¡æ¯:\n")
            for k, v in INFO[dataset_key].items():
                f.write(f"  {k}: {v}\n")

        print("\nâœ… æ•°æ®é›†å¤„ç†å®Œæˆï¼")
        print(f"   ğŸ“ åŸå§‹: {raw_dir}")
        print(f"   ğŸ“ ImageFolder: {imagefolder_dir}")
        print(f"   ğŸ“„ labels.json / dataset_info.txt å·²ç”Ÿæˆ")
        print(f"\nç›®å½•é¢„è§ˆï¼š\n  {imagefolder_dir}/train|val|test/class_*/xxx.png")
        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½/è½¬æ¢å¤±è´¥: {e}")
        import traceback; traceback.print_exc()
        return False

def download_all_datasets(output_base_dir, raw_only=False):
    print("\n" + "=" * 80)
    print("å¼€å§‹ä¸‹è½½æ‰€æœ‰ MedMNIST æ•°æ®é›†".center(80))
    print("=" * 80)
    succ, fail = 0, []
    for key in DATASET_MAP.keys():
        ok = download_dataset(key, os.path.join(output_base_dir, key), raw_only)
        succ += int(ok)
        if not ok: fail.append(key)
        print()
    print("=" * 80)
    print(f"ä¸‹è½½å®Œæˆï¼šæˆåŠŸ {succ}/{len(DATASET_MAP)}")
    if fail: print("å¤±è´¥ï¼š", ", ".join(fail))
    print("=" * 80)

def main():
    ap = argparse.ArgumentParser(
        description="MedMNIST ä¸‹è½½/è½¬æ¢å·¥å…·ï¼ˆImageFolder-224ï¼‰"
    )
    ap.add_argument("--info", action="store_true", help="æ˜¾ç¤ºå¯ç”¨æ•°æ®é›†ä¿¡æ¯")
    ap.add_argument("--dataset", type=str, choices=list(DATASET_MAP.keys()),
                    help="æŒ‡å®šä¸‹è½½çš„æ•°æ®é›†é”®å")
    ap.add_argument("--all", action="store_true", help="ä¸‹è½½æ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†")
    ap.add_argument("--output", type=str, default="./medmnist_data",
                    help="è¾“å‡ºæ ¹ç›®å½•ï¼ˆé»˜è®¤ ./medmnist_dataï¼‰")
    ap.add_argument("--raw-only", action="store_true", help="ä»…ä¸‹è½½åŸå§‹æ•°æ®ï¼Œä¸åšè½¬æ¢")
    args = ap.parse_args()

    if args.info:
        print_dataset_info()
        return

    if not args.dataset and not args.all:
        print("âŒ éœ€è¦ --dataset æˆ– --allï¼›ç”¨ --info æŸ¥çœ‹æ•°æ®é›†åˆ—è¡¨")
        sys.exit(1)

    out_root = os.path.abspath(args.output)
    os.makedirs(out_root, exist_ok=True)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {out_root}")

    if args.all:
        download_all_datasets(out_root, args.raw_only)
    else:
        ds_root = os.path.join(out_root, args.dataset.lower())
        download_dataset(args.dataset.lower(), ds_root, args.raw_only)

    print("\nğŸ‰ å®Œæˆï¼\n")

if __name__ == "__main__":
    main()
